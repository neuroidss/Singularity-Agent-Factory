
import { pipeline, type TextGenerationPipeline } from '@huggingface/transformers';
import type { APIConfig } from '../types';

let generator: TextGenerationPipeline | null = null;
let currentModelId: string | null = null;
let currentDevice: string | null = null;

const handleProgress = (onProgress: (message: string) => void) => (progress: any) => {
     const { status, file, progress: p, loaded, total } = progress;
     if (status === 'progress' && p > 0) {
         const friendlyLoaded = (loaded / 1024 / 1024).toFixed(1);
         const friendlyTotal = (total / 1024 / 1024).toFixed(1);
         onProgress(`–ó–∞–≥—Ä—É–∑–∫–∞ ${file}: ${Math.round(p)}% (${friendlyLoaded}MB / ${friendlyTotal}MB)`);
     } else if (status !== 'progress') {
         onProgress(`–°—Ç–∞—Ç—É—Å: ${status}...`);
     }
};

const getPipeline = async (modelId: string, onProgress: (message: string) => void): Promise<TextGenerationPipeline> => {
    // In this simplified version, device is hardcoded, but can be expanded via a config.
    const huggingFaceDevice = 'webgpu'; 

    if (generator && currentModelId === modelId && currentDevice === huggingFaceDevice) {
        return generator;
    }

    onProgress(`üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: ${modelId}. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...`);
    
    if (generator) {
        await generator.dispose();
        generator = null;
    }

    (window as any).env = (window as any).env || {};
    (window as any).env.allowLocalModels = false;
    (window as any).env.useFbgemm = false;
    
    // By typing options as 'any', we prevent TypeScript from creating a massive union type
    // from all the pipeline() overloads, which was causing a "type is too complex" error.
    const pipelineOptions = {
        device: huggingFaceDevice,
        progress_callback: handleProgress(onProgress),
        quantization: 'fp16'
    };
    
    generator = await pipeline('text-generation', modelId, pipelineOptions as any) as TextGenerationPipeline;

    currentModelId = modelId;
    currentDevice = huggingFaceDevice;
    
    onProgress(`‚úÖ –ú–æ–¥–µ–ª—å ${modelId} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.`);
    return generator;
};

const executePipe = async (pipe: TextGenerationPipeline, system: string, user:string, temp: number): Promise<string> => {
    const prompt = `<|system|>\n${system}<|end|>\n<|user|>\n${user}<|end|>\n<|assistant|>`;

    const outputs = await pipe(prompt, {
        max_new_tokens: 2048,
        temperature: temp > 0 ? temp : 0.1,
        do_sample: temp > 0,
        top_k: temp > 0 ? 50 : 1,
    });
    
    const rawText = (outputs[0] as any).generated_text;
    const assistantResponse = rawText.split('<|assistant|>').pop()?.trim();

    if (!assistantResponse) {
        throw new Error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–∑ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏.");
    }
    
    return assistantResponse;
};

const generateDetailedError = (error: unknown, modelId: string, rawResponse?: string): Error => {
    let finalMessage;
    if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
        finalMessage = `–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –¥–ª—è ${modelId}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –±–ª–æ–∫–∏—Ä–æ–≤—â–∏–∫–∏ —Ä–µ–∫–ª–∞–º—ã.`;
    } else {
        finalMessage = `–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ HuggingFace (${modelId}): ${error instanceof Error ? error.message : "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"}`;
    }
    const processingError = new Error(finalMessage) as any;
    processingError.rawAIResponse = rawResponse || "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç.";
    return processingError;
};

const generate = async (system: string, user: string, modelId: string, temperature: number, onProgress: (message: string) => void): Promise<string> => {
     try {
        const pipe = await getPipeline(modelId, onProgress);
        const responseText = await executePipe(pipe, system, user, temperature);
        return responseText;
    } catch (e) {
        throw generateDetailedError(e, modelId);
    }
};

export const generateJsonOutput = async (
    userInput: string,
    systemInstruction: string,
    modelId: string,
    temperature: number,
    apiConfig: APIConfig,
    onProgress: (message: string) => void,
): Promise<string> => {
    const fullSystemInstruction = `${systemInstruction}\n\n–í—ã –î–û–õ–ñ–ù–´ –æ—Ç–≤–µ—Ç–∏—Ç—å –æ–¥–Ω–∏–º, –≤–∞–ª–∏–¥–Ω—ã–º JSON-–æ–±—ä–µ–∫—Ç–æ–º –∏ –Ω–∏—á–µ–º –±–æ–ª—å—à–µ. –ù–µ –æ–±–æ—Ä–∞—á–∏–≤–∞–π—Ç–µ JSON –≤ —Ç—Ä–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏.`;
    const responseText = await generate(fullSystemInstruction, userInput, modelId, temperature, onProgress);
    return responseText || "{}";
};

export const generateText = async (
    userInput: string,
    systemInstruction: string,
    modelId: string,
    temperature: number,
    apiConfig: APIConfig,
    onProgress: (message: string) => void
): Promise<string> => {
    return await generate(systemInstruction, userInput, modelId, temperature, onProgress);
};